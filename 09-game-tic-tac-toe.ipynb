{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNi+VlrrBEScZKcgGCJ0TBG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPT28JJqET0B","executionInfo":{"status":"ok","timestamp":1729670149163,"user_tz":360,"elapsed":148816,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"7330b530-17be-4a8c-d7fb-399cec37c34e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iteration 1/5\n","Epoch 1/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0734 - loss: 0.4084\n","Epoch 2/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1375 - loss: 0.3725 \n","Epoch 3/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1808 - loss: 0.4403 \n","Epoch 4/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1656 - loss: 0.3136 \n","Epoch 5/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1737 - loss: 0.2795  \n","Epoch 6/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1249 - loss: 0.1096 \n","Epoch 7/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1412 - loss: -0.0312 \n","Epoch 8/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1484 - loss: -0.4341 \n","Epoch 9/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1458 - loss: -0.7802\n","Epoch 10/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1561 - loss: -1.7972 \n","Test board state: [1, -1, 0, 1, 0, -1, 0, 0, 0]\n","Predicted next move: 7\n","\n","\n","Iteration 2/5\n","Epoch 1/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0430 - loss: -0.6656     \n","Epoch 2/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0966 - loss: -1.0531 \n","Epoch 3/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0933 - loss: -2.1991 \n","Epoch 4/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0910 - loss: -6.2236  \n","Epoch 5/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0994 - loss: -9.9367 \n","Epoch 6/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0872 - loss: -17.9260  \n","Epoch 7/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0993 - loss: -34.0211\n","Epoch 8/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1199 - loss: -40.7774  \n","Epoch 9/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1205 - loss: -110.2980 \n","Epoch 10/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0933 - loss: -169.0858\n","Test board state: [1, -1, 0, 1, 0, -1, 0, 0, 0]\n","Predicted next move: 6\n","\n","\n","Iteration 3/5\n","Epoch 1/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0028 - loss: 45.8298      \n","Epoch 2/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0026 - loss: 32.2279      \n","Epoch 3/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0012 - loss: -149.1181     \n","Epoch 4/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0106 - loss: -152.9476    \n","Epoch 5/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0381 - loss: -135.7800\n","Epoch 6/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0500 - loss: -305.2787     \n","Epoch 7/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0709 - loss: -334.2202 \n","Epoch 8/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0868 - loss: -451.9351 \n","Epoch 9/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0682 - loss: -581.8205 \n","Epoch 10/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0764 - loss: -515.5806 \n","Test board state: [1, -1, 0, 1, 0, -1, 0, 0, 0]\n","Predicted next move: 6\n","\n","\n","Iteration 4/5\n","Epoch 1/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0177 - loss: -427.7821 \n","Epoch 2/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0564 - loss: -280.3796\n","Epoch 3/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0647 - loss: -67.4966  \n","Epoch 4/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0569 - loss: -1033.2535\n","Epoch 5/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0719 - loss: -1786.6802  \n","Epoch 6/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0555 - loss: -2202.4900 \n","Epoch 7/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0568 - loss: -2728.9553 \n","Epoch 8/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0777 - loss: -2998.8596 \n","Epoch 9/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0645 - loss: -4423.8560 \n","Epoch 10/10\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0660 - loss: -5574.7407 \n","Test board state: [1, -1, 0, 1, 0, -1, 0, 0, 0]\n","Predicted next move: 6\n","\n","\n","Iteration 5/5\n","Epoch 1/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0018 - loss: 900.6494      \n","Epoch 2/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0341 - loss: 1648.7251\n","Epoch 3/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0861 - loss: 1135.0139 \n","Epoch 4/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0582 - loss: 867.5565  \n","Epoch 5/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0821 - loss: -1521.8441  \n","Epoch 6/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0475 - loss: 39.0351   \n","Epoch 7/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0587 - loss: -605.3484\n","Epoch 8/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0655 - loss: 1963.3607   \n","Epoch 9/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0609 - loss: -1631.5775\n","Epoch 10/10\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0767 - loss: -1810.2717 \n","Test board state: [1, -1, 0, 1, 0, -1, 0, 0, 0]\n","Predicted next move: 3\n","\n"]}],"source":["import numpy as np\n","import random\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Constants for Tic-Tac-Toe\n","EMPTY, PLAYER_X, PLAYER_O = 0, 1, -1\n","BOARD_SIZE = 9\n","\n","# Convert board state to input for the model\n","def board_to_input(board):\n","    return np.array(board).reshape(1, BOARD_SIZE)\n","\n","# Check if the current player has won\n","def check_winner(board):\n","    win_conditions = [(0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n","                      (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n","                      (0, 4, 8), (2, 4, 6)]             # Diagonals\n","    for wc in win_conditions:\n","        if board[wc[0]] == board[wc[1]] == board[wc[2]] != EMPTY:\n","            return board[wc[0]]\n","    return 0 if EMPTY in board else None  # Return None if draw\n","\n","# Get available moves\n","def available_moves(board):\n","    return [i for i, spot in enumerate(board) if spot == EMPTY]\n","\n","# Make a move for a player\n","def make_move(board, move, player):\n","    board[move] = player\n","\n","# Epsilon-greedy strategy for exploration\n","def choose_move(model, board, epsilon=0.1):\n","    if random.random() < epsilon:  # Exploration: random move\n","        return random.choice(available_moves(board))\n","    else:  # Exploitation: use the model to predict the best move\n","        input_data = board_to_input(board)\n","        predictions = model.predict(input_data, verbose=0)\n","        move = np.argmax(predictions[0])\n","        return move\n","\n","# Simulate a game of self-play with epsilon-greedy exploration\n","def simulate_game(model, epsilon=0.1):\n","    board = [EMPTY] * BOARD_SIZE\n","    current_player = PLAYER_X\n","    game_history = []\n","\n","    while True:\n","        if current_player == PLAYER_X:\n","            move = choose_move(model, board, epsilon)\n","        else:\n","            move = random.choice(available_moves(board))  # Random move for Player O\n","\n","        if board[move] != EMPTY:\n","            move = random.choice(available_moves(board))  # Invalid move, choose randomly\n","\n","        game_history.append((board.copy(), move, current_player))\n","        make_move(board, move, current_player)\n","\n","        winner = check_winner(board)\n","        if winner or winner is None:\n","            return game_history, winner\n","\n","        current_player = PLAYER_X if current_player == PLAYER_O else PLAYER_O\n","\n","# Create the neural network model with a deeper architecture\n","def create_model():\n","    model = tf.keras.Sequential([\n","        layers.InputLayer(input_shape=(BOARD_SIZE,)),\n","        layers.Dense(256, activation='relu'),  # Increased neurons\n","        layers.Dense(128, activation='relu'),\n","        layers.Dense(64, activation='relu'),\n","        layers.Dense(BOARD_SIZE, activation='softmax')  # Output probabilities for each position\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Collect games for training\n","def collect_games(model, num_games=100, epsilon=0.1):\n","    games = []\n","\n","    for _ in range(num_games):\n","        game_history, winner = simulate_game(model, epsilon)\n","        games.append((game_history, winner))  # Collect all games, both winning and losing\n","\n","    return games\n","\n","# Prepare training data from games with rewards for winning and losing\n","def prepare_training_data(games):\n","    boards = []\n","    next_moves = []\n","    rewards = []\n","\n","    for game, winner in games:\n","        for i in range(len(game) - 1):  # Ignore the last move since there is no next move to predict\n","            board, move, player = game[i]\n","            next_move = game[i + 1][1]  # The next move in the game history\n","\n","            if player == PLAYER_X:\n","                # Reward for winning moves\n","                reward = 1 if winner == PLAYER_X else (-1 if winner == PLAYER_O else 0)\n","                boards.append(board.copy())\n","                next_moves.append(next_move)\n","                rewards.append(reward)\n","\n","    return np.array(boards), np.array(next_moves), np.array(rewards)\n","\n","# Train the model using both winning and losing games\n","def train_model_on_games(model, games):\n","    boards, next_moves, rewards = prepare_training_data(games)\n","\n","    # Train the model, using rewards as sample weights\n","    model.fit(boards, next_moves, sample_weight=rewards, epochs=10, verbose=1)\n","\n","# Predict the next move for a given board state\n","def predict_next_move(model, board):\n","    input_data = board_to_input(board)\n","    predictions = model.predict(input_data, verbose=0)\n","    move = np.argmax(predictions[0])\n","    return move\n","\n","# Iterative learning process with improvements\n","def iterative_learning(model, iterations=5, games_per_iteration=100, test_board=None, epsilon=0.1):\n","    for iteration in range(iterations):\n","        print(f\"\\nIteration {iteration + 1}/{iterations}\")\n","\n","        # Simulate a batch of games and collect both winning and losing games\n","        games = collect_games(model, num_games=games_per_iteration, epsilon=epsilon)\n","\n","        # Train the model on the collected games\n","        train_model_on_games(model, games)\n","\n","        # Test the model on the sample test board\n","        if test_board is not None:\n","            predicted_move = predict_next_move(model, test_board)\n","            print(f\"Test board state: {test_board}\")\n","            print(f\"Predicted next move: {predicted_move}\\n\")\n","\n","# Example test board (can be modified)\n","test_board = [PLAYER_X, PLAYER_O, EMPTY, PLAYER_X, EMPTY, PLAYER_O, EMPTY, EMPTY, EMPTY]\n","\n","# Main script\n","if __name__ == '__main__':\n","    model = create_model()\n","\n","    # Perform iterative learning with a set number of iterations and epsilon-greedy exploration\n","    iterative_learning(model, iterations=5, games_per_iteration=100, test_board=test_board, epsilon=0.1)\n"]}]}